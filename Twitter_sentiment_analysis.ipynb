{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Twitter Sentiment Analysis\n",
        "\n",
        "### By Netra Mittal\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project trains a naive Bayes classifer to predict sentiments of tweets. "
      ],
      "metadata": {
        "id": "AoblouKp2OzM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7gt-_Z_-2LOm"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Download Twitter dataset\n",
        "nltk.download('twitter_samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcSJWvYp34fV",
        "outputId": "f8616084-da0d-45dd-c2be-cbc8b8749153"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## We can load the text fields of the positive and negative tweets by using the module's strings() method\n",
        "\n",
        "all_pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_neg_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "P8LO7_AP3-Rn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Print report with no. of positive and negative tweets\n",
        "\n",
        "print('Number of positive tweets : ', len(all_pos_tweets))\n",
        "print('Number of negitive tweets : ', len(all_neg_tweets))\n",
        "\n",
        "print('\\nType of all_pos_tweets is : ', type(all_pos_tweets))\n",
        "print('Type of all_neg_tweets is : ', type(all_neg_tweets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25E0MYRp4aiM",
        "outputId": "447ce146-5472-4892-f310-94f4ad69eb50"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of positive tweets :  5000\n",
            "Number of negitive tweets :  5000\n",
            "\n",
            "Type of all_pos_tweets is :  <class 'list'>\n",
            "Type of all_neg_tweets is :  <class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Examples of tweets\n",
        "\n",
        "print(\"Positive Tweet Ex: \\n\", all_pos_tweets[0])\n",
        "print(\"\\nNegative Tweet Ex: \\n\", all_neg_tweets[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMUrBxlb5Cr9",
        "outputId": "16aba609-ff2d-4aff-e1bf-ebaa1e0445b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive Tweet Ex: \n",
            " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "\n",
            "Negative Tweet Ex: \n",
            " hopeless for tmr :(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean the dataset/ Preprocess the tweets"
      ],
      "metadata": {
        "id": "p_sQrA6S5lbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer "
      ],
      "metadata": {
        "id": "q82qtuLY5i7j"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing hyperlinks, Twitter marks and styles\n",
        "\n",
        "We don't want to use every word in a tweet as many have hastags, retweet marks, and hyperlinks. We'll use regex to remove these from a tweet."
      ],
      "metadata": {
        "id": "mrfkbhd957Ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_1(tweet):\n",
        "\n",
        "  new_tweet = re.sub(r'^RT[\\s]+', '', tweet) #removing retweets\n",
        "  new_tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', new_tweet) #removing hyperlinks\n",
        "  new_tweet = re.sub(r'#', '', new_tweet) #removing hashtags \n",
        "\n",
        "  return new_tweet \n"
      ],
      "metadata": {
        "id": "_J5y4ArA56Bb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing the string\n",
        "\n",
        "Splitting the strings into words"
      ],
      "metadata": {
        "id": "RsK6wTVn7B4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#don't want to preserve capitals, handles, or repeated words\n",
        "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "\n",
        "def tokenize_tweet(tweet):\n",
        "\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "  return tweet_tokens"
      ],
      "metadata": {
        "id": "a-FgaZco7Aon"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing stop words, punctuations"
      ],
      "metadata": {
        "id": "Sz7QkSfL7lty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.sonority_sequencing import punctuation\n",
        "nltk.download('stopwords')\n",
        "\n",
        "## importing english stop words from nltk\n",
        "stopwords_eng = stopwords.words('english')\n",
        "\n",
        "punctuations = string.punctuation \n",
        "\n",
        "def remove_stopwords_punctuations(tweet_tokens):\n",
        "\n",
        "  tweets_clean = []\n",
        "\n",
        "  for word in tweet_tokens:\n",
        "    if (word not in stopwords_eng and word not in punctuations):\n",
        "      tweets_clean.append(word)\n",
        "\n",
        "  return tweets_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKGR_q907kgR",
        "outputId": "bfd1e7da-f625-4840-c71b-8fe91315b7b2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming \n",
        "\n",
        "Next, we'll convert the word into the most basic form. Ex: Learning/ Learned/ Learnt converts to the base form 'Learn'."
      ],
      "metadata": {
        "id": "GM2ULPA28uls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "def get_stem(tweets_clean):\n",
        "\n",
        "  tweets_stem = []\n",
        "\n",
        "  for word in tweets_clean:\n",
        "    stem_word = stemmer.stem(word)\n",
        "    tweets_stem.append(stem_word)\n",
        "\n",
        "  return tweets_stem"
      ],
      "metadata": {
        "id": "CUSrEqdZ8s22"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also check how a tweet is processed through each of these steps: "
      ],
      "metadata": {
        "id": "GSruc29I9wCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ex_tweet = all_pos_tweets[7]\n",
        "\n",
        "step1 = remove_1(ex_tweet)\n",
        "step2 = tokenize_tweet(step1)\n",
        "step3 = remove_stopwords_punctuations(step2)\n",
        "step4 = get_stem(step3)\n",
        "\n",
        "print(ex_tweet, '\\n\\n1. Removed hyperlinks etc. : ', step1, \n",
        "      '\\n\\n2. Tokenized tweet : ', step2,\n",
        "      '\\n\\n3. Removed stopwords : ', step3,\n",
        "      '\\n\\n4. Stem words : ', step4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m77RxlV19r3G",
        "outputId": "654ba94f-1629-47c2-d74c-44728fb4e754"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying. \n",
            "\n",
            "1. Removed hyperlinks etc. :  @Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying. \n",
            "\n",
            "2. Tokenized tweet :  ['on', 'second', 'thought', ',', 'there', '’', 's', 'just', 'not', 'enough', 'time', 'for', 'a', 'dd', ':)', 'but', 'new', 'shorts', 'entering', 'system', '.', 'sheep', 'must', 'be', 'buying', '.'] \n",
            "\n",
            "3. Removed stopwords :  ['second', 'thought', '’', 'enough', 'time', 'dd', ':)', 'new', 'shorts', 'entering', 'system', 'sheep', 'must', 'buying'] \n",
            "\n",
            "4. Stem words :  ['second', 'thought', '’', 'enough', 'time', 'dd', ':)', 'new', 'short', 'enter', 'system', 'sheep', 'must', 'buy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining all the functions into one process"
      ],
      "metadata": {
        "id": "Pv0lQVDC_V4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_tweets(tweet):\n",
        "  step1 = remove_1(tweet)\n",
        "  step2 = tokenize_tweet(step1)\n",
        "  step3 = remove_stopwords_punctuations(step2)\n",
        "  final = get_stem(step3)\n",
        "\n",
        "  return final"
      ],
      "metadata": {
        "id": "uwSLYIl4-0AG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## check function \n",
        "\n",
        "process_tweets(all_neg_tweets[7])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee9XDW_3RUTk",
        "outputId": "05d6fb72-512d-4429-ad7e-c7de94deaea2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['work', 'neighbour', 'motor', 'ask', 'said', 'hate', 'updat', 'search', ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data into training and testing sets"
      ],
      "metadata": {
        "id": "Ie_mx3A3Rff5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_pos = all_pos_tweets[4000:]\n",
        "train_pos = all_pos_tweets[:4000]\n",
        "\n",
        "test_neg = all_neg_tweets[4000:]\n",
        "train_neg = all_neg_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg))) #1-- pos, 0--neg\n",
        "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
      ],
      "metadata": {
        "id": "OQY0NSl6RadA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating frequency dictionary"
      ],
      "metadata": {
        "id": "hhMOyWQuSsN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_frequency(tweets, ys): #list of tweets and corresponding y value\n",
        "  freq_d = {}\n",
        "\n",
        "  for tweet,y in zip(tweets,ys): #iterate through each word and val\n",
        "    for word in process_tweets(tweet): #turns into list of keywords\n",
        "\n",
        "      pair = (word, y) #tuple of the word and the y value\n",
        "\n",
        "      if pair in freq_d: #if the pair is already in the dictionary, increase val by 1\n",
        "        freq_d[pair] += 1\n",
        "\n",
        "      else: #initialize the key value starting at one\n",
        "        freq_d[pair] = freq_d.get(pair, 1)\n",
        "\n",
        "  return freq_d"
      ],
      "metadata": {
        "id": "86APlxcES1dm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## testing the function \n",
        "\n",
        "tweets = ['i am happy', 'i was tricked', 'i am angry', 'i am excited', 'i am happy']\n",
        "ys = [1, 0, 0, 1, 1]\n",
        "\n",
        "freq_d = create_frequency(tweets, ys)\n",
        "print(freq_d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZpJz8_3ULOW",
        "outputId": "9838e222-94be-4b6c-afb1-10c20730b8c0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('happi', 1): 2, ('trick', 0): 1, ('angri', 0): 1, ('excit', 1): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model using Naive Bayes"
      ],
      "metadata": {
        "id": "gA4NnLlXU_7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "  '''\n",
        "  Inputs:\n",
        "    freqs: dict from (word, label) for how often a word appears\n",
        "    train_x: list of tweets\n",
        "    train_y: list of labels corresponding to the tweets\n",
        "  Outputs:\n",
        "    logprior: log prior\n",
        "    loglikelihood: log likelihood of naive bayes eq\n",
        "\n",
        "  '''\n",
        "\n",
        "  loglikelihood = {}\n",
        "  logprior = 0\n",
        "\n",
        "  #calculate unique words\n",
        "  unique_words = set([pair[0] for pair in freqs.keys()])\n",
        "  V = len(unique_words)\n",
        "\n",
        "  #calculate N_pos and N_neg words\n",
        "  N_pos = N_neg = 0\n",
        "  for pair in freqs.keys():\n",
        "    if pair[1] > 0: #if label is positive\n",
        "      N_pos += freqs[(pair)]\n",
        "\n",
        "    else: #neg label\n",
        "      N_neg += freqs[(pair)]\n",
        "\n",
        "    D = train_y.shape[0] #total tweets\n",
        "\n",
        "    D_pos = sum(train_y) #total pos tweets (note: different (<) from pos words)\n",
        "\n",
        "    D_neg = D - D_pos #total neg\n",
        "\n",
        "  logprior = np.log(D_pos) - np.log(D_neg) #log to avoid extreme values\n",
        "\n",
        "  for word in unique_words: #iterate through each unique word and get no. of times\n",
        "                            #the word appeared in pos vs neg tweets\n",
        "\n",
        "    freq_pos = freqs.get((word,1), 0) #getting frequency\n",
        "    freq_neg = freqs.get((word, 0), 0)\n",
        "\n",
        "    p_w_pos = (freq_pos + 1) / (N_pos + V) #getting probability \n",
        "    p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
        "\n",
        "    loglikelihood[word] = np.log(p_w_pos / p_w_neg) #log likelihood of a word\n",
        "    #this tells us that if loglikelihood > 1 --> word is likely positive\n",
        "\n",
        "  return logprior, loglikelihood"
      ],
      "metadata": {
        "id": "eFW0J6yzUiMm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freqs = create_frequency(train_x, train_y)\n",
        "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
        "\n",
        "print(logprior, '\\n', len(loglikelihood))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_2us9Y7VmO9",
        "outputId": "5a9bec52-a339-4636-ad81-7b081cb17e7d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0 \n",
            " 9085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting tweets"
      ],
      "metadata": {
        "id": "khJY9upbhh-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
        "  '''\n",
        "  Output: \n",
        "    p: sum of all the loglikelihoods + logprior\n",
        "\n",
        "  '''\n",
        "\n",
        "  word_l = process_tweets(tweet)\n",
        "\n",
        "  p = 0 #initialize prob to 0\n",
        "\n",
        "  p += logprior\n",
        "\n",
        "  for word in word_l:\n",
        "    if word in loglikelihood:\n",
        "      p += loglikelihood[word]\n",
        "\n",
        "  return p"
      ],
      "metadata": {
        "id": "KAPSl7zHfHDK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_twt = ['I am happy that I went for a walk today!', 'Good good great', 'Bad bad horrible', 'That movie was horribly depressing', 'I hope my wish to study at Harvard comes true!', 'It was immaculate. Exceptional taste. Exceptional, reveal.']\n",
        "\n",
        "for tweet in list_of_twt:\n",
        "  p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
        "\n",
        "  print(f'{tweet} --> {p:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teb6-iskiR0U",
        "outputId": "b758ebb8-c790-4581-ca6c-9ba20c985c46"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am happy that I went for a walk today! --> 1.66\n",
            "Good good great --> 3.80\n",
            "Bad bad horrible --> -4.09\n",
            "That movie was horribly depressing --> -3.10\n",
            "I hope my wish to study at Harvard comes true! --> -0.36\n",
            "It was immaculate. Exceptional taste. Exceptional, reveal. --> -0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that longer tweets have a higher chance of being diagnosed wrong. Meanwhile, short tweets with recurring key words have a greater chance of being diagnosed correctly. "
      ],
      "metadata": {
        "id": "poBt41dfjfHY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cd_0uJb-j1X4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}